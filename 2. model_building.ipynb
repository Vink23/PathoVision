{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Model Building and Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn import svm\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics\n",
    "import cv2 as cv\n",
    "from utils import *\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "import xgboost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Load in Processed Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cwd = os.getcwd()\n",
    "data_path = os.path.join(cwd,'standardized_data/Training')\n",
    "\n",
    "X = []\n",
    "y = []\n",
    "\n",
    "for label in os.listdir(data_path):\n",
    "    for image in os.listdir(os.path.join(data_path,label)):\n",
    "        image_path = os.path.join(data_path,label,image)\n",
    "        load_image = cv.imread(image_path, cv.IMREAD_GRAYSCALE)\n",
    "        # flatten_image = load_image.flatten()\n",
    "        X.append(load_image)\n",
    "        y.append(label)\n",
    "\n",
    "\n",
    "# Convert to Numpy Array\n",
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "le = LabelEncoder()\n",
    "le.fit(y) \n",
    "y = le.transform(y)\n",
    "\n",
    "print(X.shape)\n",
    "print(y.shape)\n",
    "\n",
    "print(X[0])\n",
    "print(y[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Base Model (most common class classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(5,5))\n",
    "y_train_series = pd.Series(y)\n",
    "y_train_series.value_counts().plot(kind='bar')\n",
    "plt.title('Distribution of Cancer Classes')\n",
    "plt.xlabel('Class')\n",
    "plt.ylabel('Count')\n",
    "plt.show()\n",
    "print(le.classes_)\n",
    "y_transformed = le.inverse_transform(y)\n",
    "\n",
    "plt.figure(figsize=(5,5))\n",
    "y_train_series = pd.Series(y_transformed)\n",
    "y_train_series.value_counts().plot(kind='bar')\n",
    "plt.title('Distribution of Cancer Classes')\n",
    "plt.xlabel('Class')\n",
    "plt.ylabel('Count')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Generate Model Statistics\n",
    "most_common_label = stats.mode(y)[0]\n",
    "print(\"Most Common Label: \" + str(most_common_label))\n",
    "y_predict = np.ones((y.shape)) * most_common_label\n",
    "print(y_predict.shape)\n",
    "print(y.shape)\n",
    "\n",
    "# Generate Confusion Matrix\n",
    "confusion_matrix = metrics.confusion_matrix(y, y_predict)\n",
    "cm_display = metrics.ConfusionMatrixDisplay(confusion_matrix = confusion_matrix, display_labels = le.classes_)\n",
    "cm_display.plot()\n",
    "plt.show()\n",
    "\n",
    "accuracy_score = metrics.accuracy_score(y, y_predict)\n",
    "macro_precision = metrics.precision_score(y,y_predict,average ='macro')\n",
    "macro_recall = metrics.recall_score(y,y_predict,average='macro')\n",
    "macro_f1 = metrics.f1_score(y,y_predict,average='macro')\n",
    "micro_precision = metrics.precision_score(y,y_predict,average='micro')\n",
    "micro_recall = metrics.recall_score(y,y_predict,average='micro')\n",
    "micro_f1 = metrics.f1_score(y,y_predict,average='micro')\n",
    "\n",
    "print(\"================== TRAINING METRICS ===================\")\n",
    "print(\"Accuracy Score: \" + str(accuracy_score))\n",
    "print(\"Macro Precision: \" + str(macro_precision))\n",
    "print(\"Macro Recall: \" + str(macro_recall))\n",
    "print(\"Macro F1: \" + str(macro_f1))\n",
    "print(\"Micro Precision: \" + str(micro_precision))\n",
    "print(\"Micro Recall: \" + str(micro_recall))\n",
    "print(\"Micro F1: \" + str(micro_f1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Extract Features (Canny Edges, Difference of Gaussians, Complex Feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### TBD\n",
    "# X_dog_features = get_features(X, feat_name='blob_dog')\n",
    "# X_doh_features = get_features(X, feat_name='blob_doh')\n",
    "# X_canny_features = get_features(X, feat_name='canny')\n",
    "# pca = PCA(n_components=1000, svd_solver=\"randomized\", whiten=True).fit(X_canny_features)\n",
    "# X_pca_canny_features = pca.transform(X_canny_features)\n",
    "# # X_complex_features = get_features(X, feat_name='complex')\n",
    "\n",
    "# np.save('X_train_dog_features.npy', X_dog_features)\n",
    "# np.save('X_train_doh_features.npy', X_doh_features)\n",
    "# np.save('X_train_pca_canny_features.npy', X_pca_canny_features)\n",
    "# # np.save('X_train_complex_features.npy', X_complex_features)\n",
    "# np.save('Y_train.npy',y)\n",
    "# y_raw_train = le.inverse_transform(y)\n",
    "# np.save('Y_raw_train.npy',y_raw_train)\n",
    "# print(y.shape)\n",
    "# print(y_raw_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3a. Load Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_dog_features = np.load('X_train_dog_features.npy')\n",
    "X_train_doh_features = np.load('X_train_doh_features.npy')\n",
    "X_train_pca_canny_features = np.load('X_train_pca_canny_features.npy')\n",
    "bundle = get_features(None, \"complex\", joblib_path=\"complex_feat_training.joblib\", return_bundle=True)\n",
    "X_train_complex_features = bundle.X\n",
    "y_train = np.load('Y_train.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3a. PCA Variance Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x = plot_PCA([X_dog_features, X_doh_features, X_canny_features, X_complex_features], n_components=[min(X_dog_features.shape), min(X_doh_features.shape), 1000, min(X_complex_features.shape)])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Shuffle Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(281)\n",
    "\n",
    "# Create a list of indexes that is the length of the number of training examples\n",
    "indices = [i for i in range(0,X.shape[0])]\n",
    "shuffle_indices = np.random.permutation(indices)\n",
    "\n",
    "# Reorder X and Y based on the shuffled indices\n",
    "# X = X[shuffle_indices]\n",
    "# y = y[shuffle_indices]\n",
    "\n",
    "X_dog_features = X_train_dog_features[shuffle_indices]\n",
    "X_doh_features = X_train_doh_features[shuffle_indices]\n",
    "X_canny_features = X_train_pca_canny_features[shuffle_indices]\n",
    "X_complex_features = X_train_complex_features[shuffle_indices]\n",
    "\n",
    "y_train = y_train[shuffle_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_dog_features.shape)\n",
    "print(X_dog_features.shape)\n",
    "print(X_canny_features.shape)\n",
    "print(X_complex_features.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_types = ['logistic','svm','rf','lda','qda']\n",
    "features = {'dog':X_dog_features,\n",
    "            'doh':X_doh_features,\n",
    "            'canny':X_canny_features,\n",
    "            'complex':X_complex_features}\n",
    "\n",
    "all_results = pd.DataFrame(columns = ['feature', 'model_type', 'accuracy_score', 'macro_precision', 'macro_recall', 'macro_f1', \n",
    "                                      'micro_precision', 'micro_recall', 'micro_f1', 'training_time'])\n",
    "model_store = {}\n",
    "\n",
    "for model_type in model_types:\n",
    "    for feature in features.keys():\n",
    "        model, results = train_model(features[feature], y_train, classes=le.classes_, model_type=model_type, feature=feature)\n",
    "        all_results.loc[len(all_results)] = results\n",
    "        model_store[str(model_type) + \"_\" + str(feature)] = model\n",
    "    save_models(model_store, \"models\")\n",
    "\n",
    "print(all_results)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_results.to_csv('training_results.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5a. Load Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_types = ['logistic','svm','rf','lda','qda']\n",
    "features = {'dog':X_dog_features,\n",
    "            'doh':X_doh_features,\n",
    "            'canny':X_canny_features,\n",
    "            'complex':X_complex_features}\n",
    "\n",
    "loaded_models = {}\n",
    "\n",
    "for model_type in model_types:\n",
    "    for feature in features.keys():\n",
    "        feature_model = str(model_type) + \"_\" + str(feature)\n",
    "        try:\n",
    "            loaded_models[feature_model] = joblib.load(\"models//\" + str(feature_model) + \".joblib\")\n",
    "        except:\n",
    "            print(\"Could not load \" + str(feature_model))\n",
    "\n",
    "print(loaded_models.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Final Test Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_canny = np.load('X_test_canny.npy')\n",
    "X_test_doh = np.load('X_test_doh.npy')\n",
    "X_test_dog = np.load('X_test_dog.npy')\n",
    "bundle = get_features(None, \"complex\", joblib_path=\"complex_feat_testing.joblib\", return_bundle=True)\n",
    "X_test_complex = bundle.X\n",
    "Y_test = np.load('y_test.npy')\n",
    "\n",
    "\n",
    "all_test_results= pd.DataFrame(columns = ['feature', 'model_type', 'accuracy_score', 'macro_precision', 'macro_recall', 'macro_f1', \n",
    "                                      'micro_precision', 'micro_recall', 'micro_f1', 'inference_time'])\n",
    "\n",
    "for model in loaded_models.keys():\n",
    "    if 'canny' in model:\n",
    "        X_test = X_test_canny\n",
    "    elif 'dog' in model:\n",
    "        X_test = X_test_dog\n",
    "    elif 'doh' in model:\n",
    "        X_test = X_test_doh\n",
    "    elif 'complex' in model:\n",
    "        X_test = X_test_complex\n",
    "\n",
    "    feature = model.split('_')[-1]\n",
    "    model_type = model.split('_')[0]\n",
    "\n",
    "    result_dict = test_model(model_store[model], X_test, Y_test, le.classes_, model_type=str(model_type), feature=str(feature))\n",
    "    all_test_results.loc[len(all_test_results)] = result_dict\n",
    "\n",
    "all_test_results.to_csv(\"test_results.csv\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "datasci281",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
